ssl_tools.experiments.har_classification.tnc
============================================

.. py:module:: ssl_tools.experiments.har_classification.tnc


Attributes
----------

.. autoapisummary::

   ssl_tools.experiments.har_classification.tnc.options


Classes
-------

.. autoapisummary::

   ssl_tools.experiments.har_classification.tnc.TNCTest
   ssl_tools.experiments.har_classification.tnc.TNCTrain


Module Contents
---------------

.. py:class:: TNCTest(data, encoding_size = 10, in_channel = 6, window_size = 60, mc_sample_size = 20, w = 0.05, num_classes = 6, *args, **kwargs)

   Bases: :py:obj:`ssl_tools.experiments.LightningTest`


   Helper class that provides a standard way to create an ABC using
   inheritance.

   Trains the constrastive predictive coding model

   Parameters
   ----------
   encoding_size : int, optional
       Size of the encoding (output of the linear layer)
   in_channel : int, optional
       Number of channels in the input data
   window_size : int, optional
       Size of the input windows (X_t) to be fed to the encoder
   pad_length : bool, optional
       If True, the samples are padded to the length of the longest sample
       in the dataset.


   .. py:attribute:: _MODEL_NAME
      :value: 'TNC'



   .. py:method:: get_data_module()

      Get the datamodule to use for the experiment.

      Returns
      -------
      L.LightningDataModule
          The datamodule to use for the experiment



   .. py:method:: get_model()

      Get the model to use for the experiment.

      Returns
      -------
      L.LightningModule
          The model to use for the experiment



.. py:class:: TNCTrain(data, encoding_size = 10, in_channel = 6, window_size = 60, mc_sample_size = 20, w = 0.05, significance_level = 0.01, repeat = 5, pad_length = True, num_classes = 6, update_backbone = False, *args, **kwargs)

   Bases: :py:obj:`ssl_tools.experiments.LightningSSLTrain`


   Helper class that provides a standard way to create an ABC using
   inheritance.

   Trains the constrastive predictive coding model

   Parameters
   ----------
   encoding_size : int, optional
       Size of the encoding (output of the linear layer)
   in_channel : int, optional
       Number of channels in the input data
   window_size : int, optional
       Size of the input windows (X_t) to be fed to the encoder
   pad_length : bool, optional
       If True, the samples are padded to the length of the longest sample
       in the dataset.
   num_classes : int, optional
       Number of classes in the dataset. Only used in finetune mode.
   update_backbone : bool, optional
       If True, the backbone will be updated during training. Only used in
       finetune mode.


   .. py:attribute:: _MODEL_NAME
      :value: 'TNC'



   .. py:method:: get_finetune_data_module()

      The data module to use for fine-tuning.

      Returns
      -------
      L.LightningDataModule
          The data module to use for fine-tuning

      Raises
      ------
      NotImplementedError
          _description_



   .. py:method:: get_finetune_model(load_backbone = None)

      Get the model to use for fine-tuning.

      Parameters
      ----------
      load_backbone : str, optional
          The path to the backbone to load. The backbone must be loaded
          inside this method, if it is not None.

      Returns
      -------
      L.LightningModule
          The model to use for fine-tuning



   .. py:method:: get_pretrain_data_module()

      The data module to use for pre-training.

      Returns
      -------
      L.LightningDataModule
          The data module to use for pre-training



   .. py:method:: get_pretrain_model()

      Get the model to use for the pretraining phase.

      Returns
      -------
      L.LightningModule
          The model to use for the pretraining phase



.. py:data:: options

