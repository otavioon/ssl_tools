ssl_tools.pipelines.mlflow_train
================================

.. py:module:: ssl_tools.pipelines.mlflow_train


Classes
-------

.. autoapisummary::

   ssl_tools.pipelines.mlflow_train.LightningFineTuneMLFlow
   ssl_tools.pipelines.mlflow_train.LightningTrainMLFlow


Module Contents
---------------

.. py:class:: LightningFineTuneMLFlow(registered_model_name, registered_model_tags = None, update_backbone = False, **kwargs)

   Bases: :py:obj:`LightningTrainMLFlow`


   
   Train a model using Lightning framework.

   Parameters
   ----------
   experiment_name : str
       Name of the experiment.
   model_name : str
       Name of the model.
   dataset_name : str
       Name of the dataset.
   run_name : str, optional
       The name of the run, by default None
   accelerator : str, optional
       The accelerator to use, by default "cpu"
   devices : int, optional
       Number of accelerators to use, by default 1
   num_nodes : int, optional
       Number of nodes, by default 1
   strategy : str, optional
       Training strategy, by default "auto"
   max_epochs : int, optional
       Maximium number of epochs, by default 1
   batch_size : int, optional
       Batch size, by default 1
   limit_train_batches : int | float, optional
       Limit the number of batches to train, by default 1.0
   limit_val_batches : int | float, optional
       Limit the number of batches to test, by default 1.0
   checkpoint_monitor_metric : str, optional
       The metric to monitor for checkpointing, by default None
   checkpoint_monitor_mode : str, optional
       The mode for checkpointing, by default "min"
   patience : int, optional
       The patience for early stopping, by default None
   log_dir : str, optional
       Location where logs will be saved, by default "./runs"


   .. py:property:: client


   .. py:method:: load_model()


.. py:class:: LightningTrainMLFlow(experiment_name, model_name, run_name = None, accelerator = 'cpu', devices = 1, num_nodes = 1, strategy = 'auto', max_epochs = 1, batch_size = 1, limit_train_batches = 1.0, limit_val_batches = 1.0, checkpoint_monitor_metric = None, checkpoint_monitor_mode = 'min', patience = None, log_dir = './mlruns', model_tags = None)

   Bases: :py:obj:`ssl_tools.pipelines.base.Pipeline`


   
   Train a model using Lightning framework.

   Parameters
   ----------
   experiment_name : str
       Name of the experiment.
   model_name : str
       Name of the model.
   dataset_name : str
       Name of the dataset.
   run_name : str, optional
       The name of the run, by default None
   accelerator : str, optional
       The accelerator to use, by default "cpu"
   devices : int, optional
       Number of accelerators to use, by default 1
   num_nodes : int, optional
       Number of nodes, by default 1
   strategy : str, optional
       Training strategy, by default "auto"
   max_epochs : int, optional
       Maximium number of epochs, by default 1
   batch_size : int, optional
       Batch size, by default 1
   limit_train_batches : int | float, optional
       Limit the number of batches to train, by default 1.0
   limit_val_batches : int | float, optional
       Limit the number of batches to test, by default 1.0
   checkpoint_monitor_metric : str, optional
       The metric to monitor for checkpointing, by default None
   checkpoint_monitor_mode : str, optional
       The mode for checkpointing, by default "min"
   patience : int, optional
       The patience for early stopping, by default None
   log_dir : str, optional
       Location where logs will be saved, by default "./runs"


   .. py:method:: get_callbacks()


   .. py:method:: get_data_module()
      :abstractmethod:



   .. py:method:: get_logger()


   .. py:method:: get_model()
      :abstractmethod:



   .. py:method:: get_trainer(logger, callacks)


   .. py:method:: run()


