ssl_tools.pipelines.har_classification.tnc
==========================================

.. py:module:: ssl_tools.pipelines.har_classification.tnc


Attributes
----------

.. autoapisummary::

   ssl_tools.pipelines.har_classification.tnc.options


Classes
-------

.. autoapisummary::

   ssl_tools.pipelines.har_classification.tnc.TNCFineTune
   ssl_tools.pipelines.har_classification.tnc.TNCPreTrain


Module Contents
---------------

.. py:class:: TNCFineTune(data, num_classes = 6, num_workers = None, **kwargs)

   Bases: :py:obj:`ssl_tools.pipelines.mlflow_train.LightningFineTuneMLFlow`


   
   Train a model using Lightning framework.

   Parameters
   ----------
   experiment_name : str
       Name of the experiment.
   model_name : str
       Name of the model.
   dataset_name : str
       Name of the dataset.
   run_name : str, optional
       The name of the run, by default None
   accelerator : str, optional
       The accelerator to use, by default "cpu"
   devices : int, optional
       Number of accelerators to use, by default 1
   num_nodes : int, optional
       Number of nodes, by default 1
   strategy : str, optional
       Training strategy, by default "auto"
   max_epochs : int, optional
       Maximium number of epochs, by default 1
   batch_size : int, optional
       Batch size, by default 1
   limit_train_batches : int | float, optional
       Limit the number of batches to train, by default 1.0
   limit_val_batches : int | float, optional
       Limit the number of batches to test, by default 1.0
   checkpoint_monitor_metric : str, optional
       The metric to monitor for checkpointing, by default None
   checkpoint_monitor_mode : str, optional
       The mode for checkpointing, by default "min"
   patience : int, optional
       The patience for early stopping, by default None
   log_dir : str, optional
       Location where logs will be saved, by default "./runs"


   .. py:method:: get_data_module()


   .. py:method:: get_model()


.. py:class:: TNCPreTrain(data, encoding_size = 10, in_channel = 6, window_size = 60, mc_sample_size = 20, w = 0.05, significance_level = 0.01, repeat = 5, pad_length = True, num_classes = 6, num_workers = None, **kwargs)

   Bases: :py:obj:`ssl_tools.pipelines.mlflow_train.LightningTrainMLFlow`


   
   Train a model using Lightning framework.

   Parameters
   ----------
   experiment_name : str
       Name of the experiment.
   model_name : str
       Name of the model.
   dataset_name : str
       Name of the dataset.
   run_name : str, optional
       The name of the run, by default None
   accelerator : str, optional
       The accelerator to use, by default "cpu"
   devices : int, optional
       Number of accelerators to use, by default 1
   num_nodes : int, optional
       Number of nodes, by default 1
   strategy : str, optional
       Training strategy, by default "auto"
   max_epochs : int, optional
       Maximium number of epochs, by default 1
   batch_size : int, optional
       Batch size, by default 1
   limit_train_batches : int | float, optional
       Limit the number of batches to train, by default 1.0
   limit_val_batches : int | float, optional
       Limit the number of batches to test, by default 1.0
   checkpoint_monitor_metric : str, optional
       The metric to monitor for checkpointing, by default None
   checkpoint_monitor_mode : str, optional
       The mode for checkpointing, by default "min"
   patience : int, optional
       The patience for early stopping, by default None
   log_dir : str, optional
       Location where logs will be saved, by default "./runs"


   .. py:method:: get_data_module()


   .. py:method:: get_model()


.. py:data:: options

